{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33a12a91",
   "metadata": {},
   "source": [
    "# Forecasting with Facebook's Prophet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73c61e52",
   "metadata": {},
   "source": [
    "Snowpark API Reference\n",
    "https://docs.snowflake.com/developer-guide/snowpark/reference/python/index.html\n",
    "\n",
    "This notebook will walk us through an example flow of a Data Scientist experimenting locally but offloading processing to Snowflake and storing the final results in Snowflake"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e063615",
   "metadata": {},
   "source": [
    "The goal of this notebook is to explore how to leverage [Facebook's Prophet](https://facebook.github.io/prophet/) Python package to create powerful forecasts completely within Snowpark.\n",
    "\n",
    "<b>Forecasting</b> - whether it's demand, sales, supply, or transportation - is a business planning method that helps analysts and planners better anticipate what is going to happen in their business. Depending on the complexity of the business operations, legacy forecasting can be within a handful of spreadsheets, in an ERP planning software (like SAP APO), or as complex as a machine learning model. \n",
    "\n",
    "For time-series analyses, several years of historical data is generally required to ensure high accuracy, depending on your situation. If you are forecasting hourly sales at a fast-food restaurant, you may need several months of hourly data to meet requirements. If you are forecasting at a weekly level for store sales, you may need several years of data to appropriately reflect any type of annual seasonality.\n",
    "\n",
    "Another critical consideration in forecasting is the level of granularity. For example, you may need a sales forecast at both a regional and a store level. Often times, there needs to be a balance between the best level of detail to make decisions off of and how what historical values were regularly measured. As a rule of thumb, it's more accurate to have several specific forecasts (one for each store) and then aggregate up to the bigger picture (sales for all stores) if need be. Creating the underlying forecast at the store level and then aggregating up to the regional level can deliver the best results.\n",
    "\n",
    "This notebook is leveraging the [Store-Item Demand Forecasting Challenge](https://www.kaggle.com/c/demand-forecasting-kernels-only/overview) Kaggle dataset. As a friendly reminder, this dataset is extremely clean, which is not usually the case for real-world forecasting. This dataset only contains historical sales data. While historical trends are generally the strongest indicators of what is to come, other factors, like pricing, promotions, distribution, or macroeconomic factors can influence sales.\n",
    "\n",
    "<b>Why Facebook Prophet?</b>\n",
    "Facebook Prophet is a powerful and easy to use time-series package, and you can use it in Snowpark! It does require a lot of historical and stationary data, but it does a great job combining long-term and short-term trends and picks up on seasonality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38e3c0c-b8e5-4057-bc17-e1ed92f81af5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Leveraging Snowpark's Pandas implementation to push processing directly to Snowflake\n",
    "import snowflake.connector as sc\n",
    "import snowflake.connector.pandas_tools as pt\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from snowflake.snowpark import Session\n",
    "from utils import snowpark_utils\n",
    "\n",
    "session = snowpark_utils.get_snowpark_session()\n",
    "session.use_database(\"snowpark\")\n",
    "session.use_schema(\"forecast\")\n",
    "session.use_warehouse(\"data_science\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb03f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc87f365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the sales data from local.\n",
    "import pandas as pd\n",
    "\n",
    "sales = pd.read_csv('data/train.csv')\n",
    "sales.columns = ['DATE','STORE','ITEM','SALES']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45281d3b",
   "metadata": {},
   "source": [
    "## Move processing to Snowflake by creating a Snowpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56efeb76-6ad7-44d1-b1e2-e05466dd8e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import concat, lit\n",
    "\n",
    "#load pandas dataframe as a snowpark temporary table\n",
    "sales_spdf = session.create_dataframe(sales)\n",
    "#alternatively could have used a table already within Snowflake\n",
    "#sales_spdf = session.table(\"sales\")\n",
    "\n",
    "# Adjust datatypes\n",
    "sales_spdf = sales_spdf.withColumn('DATE', sales_spdf.DATE.cast(\"date\"))\\\n",
    "                       .withColumn('STORE', sales_spdf.STORE.cast(\"string\"))\\\n",
    "                       .withColumn('ITEM', sales_spdf.ITEM.cast(\"string\"))\n",
    "\n",
    "# Create a field for the combination of store-item\n",
    "sales_spdf = sales_spdf.withColumn('STORE_ITEM', concat(sales_spdf.STORE, lit(\"_\"), sales_spdf.ITEM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335fa530-1168-4f78-bb13-9493cd9fb740",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_spdf.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04d48880",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "Before jumping into generating a forecast, let's get familiar with the data. Let's answer some of the following questions:\n",
    "* How many products are there?\n",
    "* How many stores are there?\n",
    "* How many combinations are there?\n",
    "* How many years of historical data is there? \n",
    "* Is the data stationary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28539d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are '+str(sales_spdf.select('STORE').distinct().count())+' unique stores')\n",
    "print('There are '+str(sales_spdf.select('ITEM').distinct().count())+' unique products')\n",
    "print('There are '+str(sales_spdf.select('STORE_ITEM').distinct().count())+' unique combinations for store-product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43bbac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A quick overview\n",
    "sales_spdf.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad696cb2-8f4d-496c-9a41-6988c32702ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import when, count, col\n",
    "\n",
    "#Check for nulls\n",
    "sales_spdf.select([count(when(col(c).isNull(),c)).alias(c) for c in sales_spdf.columns]).show()\n",
    "\n",
    "# Turns out that there are no null columns. Great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c732cece-001a-4022-b182-3589fd2de0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_size = sales_spdf.select('DATE', 'STORE', 'ITEM').dropDuplicates()\\\n",
    "                            .groupBy('STORE','ITEM').agg(count('*').alias('NUM_DAYS'))\n",
    "\n",
    "historical_size = historical_size.withColumn('NUM_WEEKS', historical_size.NUM_DAYS / 52)\\\n",
    "                                 .withColumn('NUM_YEARS', historical_size.NUM_DAYS / 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e8f60-871b-4b5a-bb3e-2e69ca7c6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_size.select('NUM_WEEKS', 'NUM_YEARS').describe().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1a47bd0",
   "metadata": {},
   "source": [
    "So far, the data looks very good! The range of sales and general statistics looks pretty reasonable. We have 5 years of historical data available. There are no null values. Now, let's more on and check to see if the data is overall stationary with some graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea071f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import sum\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot total daily sales\n",
    "daily_sales = sales_spdf.select('DATE', 'SALES')\n",
    "daily_sales = daily_sales.groupBy('DATE').agg(sum('SALES').alias('SALES'))\\\n",
    "                         .sort(col('DATE').asc())\n",
    "\n",
    "plt.figure(figsize=(14,8)) \n",
    "plt.plot(daily_sales.to_pandas()['DATE'], daily_sales.to_pandas()['SALES'])\n",
    "plt.title('Sales by Date')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9c1e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = '1'\n",
    "\n",
    "# Plot daily sales by store\n",
    "daily_sales_store = sales_spdf.select('DATE', 'STORE', 'SALES')\n",
    "daily_sales_store = daily_sales_store.groupBy('DATE','STORE').agg(sum('SALES').alias('SALES'))\\\n",
    "                         .sort(col('DATE').asc())\n",
    "\n",
    "filtered_daily_sales_store = daily_sales_store.filter(col('STORE')==store)\n",
    "plt.figure(figsize=(14,8)) \n",
    "plt.plot(filtered_daily_sales_store.to_pandas()['DATE'], filtered_daily_sales_store.to_pandas()['SALES'])\n",
    "plt.title('Store '+store+' Sales by Date')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff358d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = '15'\n",
    "\n",
    "# Plot daily sales - item\n",
    "daily_sales_item = sales_spdf.select('DATE', 'ITEM', 'SALES')\n",
    "daily_sales_item = daily_sales_item.groupBy('DATE', 'ITEM').agg(sum('SALES').alias('SALES'))\\\n",
    "                         .sort(col('DATE').asc())\n",
    "\n",
    "filtered_daily_sales_item = daily_sales_item.filter(col('ITEM')==item)\n",
    "plt.figure(figsize=(14,8)) \n",
    "plt.plot(filtered_daily_sales_item.to_pandas()['DATE'], filtered_daily_sales_item.to_pandas()['SALES'])\n",
    "plt.title('Item '+item+' Sales by Date')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_item = '3_44'\n",
    "\n",
    "# Plot daily sales - item\n",
    "daily_sales_store_item = sales_spdf.select('DATE', 'STORE', 'ITEM', 'STORE_ITEM', 'SALES')\n",
    "daily_sales_store_item = daily_sales_store_item.groupBy('DATE', 'STORE', 'ITEM', 'STORE_ITEM').agg(sum('SALES').alias('SALES'))\\\n",
    "                         .sort(col('DATE').asc())\n",
    "\n",
    "filtered_daily_sales_store_item = daily_sales_store_item.filter(col('STORE_ITEM')==store_item)\n",
    "plt.figure(figsize=(14,8)) \n",
    "plt.plot(filtered_daily_sales_store_item.to_pandas()['DATE'], filtered_daily_sales_store_item.to_pandas()['SALES'])\n",
    "plt.title('Store-Item '+store_item+' Sales by Date')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e883ed2",
   "metadata": {},
   "source": [
    "All of the data looks to be stationary, fantastic! Because we have the data to support it, we will build a forecast at the day-store-product level for a 90 day horizon."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4eb803b",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d76ffcd0",
   "metadata": {},
   "source": [
    "Since we have ample amount of data for this example, we can forecast at the lowest level of granularity: day-store-item. Even though each combination follows pretty similar patterns, we will still generate a unique forecast for each store-item combination. That's 500 forecasts! Don't worry, it's extremely easy and runs a lot faster than you would expect. \n",
    "\n",
    "\n",
    "Let's dive into it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4052857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import max\n",
    "from datetime import timedelta  \n",
    "\"\"\"\n",
    "Because we're going to set a 90 day forecasting horizon, we will determine the split date as 90 days back from the \n",
    "max date. Again, we are only doing this to ensure we have labeled data to determine how well our training set does.\n",
    "You can think of the first 4 years and 9 months are the training set. The testing or holdout set is the last 90 days.\n",
    "\"\"\"\n",
    "\n",
    "print('The max date is: '+str(sales_spdf.select(max('DATE') ).collect()[0][0]))\n",
    "print('90 days for testing starts on: '+str(sales_spdf.select(max('DATE') ).collect()[0][0]-timedelta(90)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7501e45a-33ae-45f2-9d77-57872458416a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the dataframe for modeling \n",
    "sales_sub = sales_spdf.select('DATE', 'STORE_ITEM', 'SALES')\n",
    "\n",
    "# Because the testing dataset doesn't have labels, let's leave out the last 90 days for the testing set\n",
    "train = sales_sub.filter(col('DATE')<='2017-10-02')\n",
    "\n",
    "# Combining sales and date columns\n",
    "train = train.withColumn(\"DATE_SALES\", concat(col('DATE'), lit(\"_\"), col('SALES')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb70ef7-0f4c-4f56-87ed-807437c866d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1303e715-37b0-4614-a1f1-b9e0f7e57efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will make and register a udf for the model training and inference\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "\n",
    "session.add_packages(\"pandas\", \"prophet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50d664f-6b0f-4b31-92a1-2781ff10e243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import udf\n",
    "from snowflake.snowpark.types import ArrayType, StringType\n",
    "\n",
    "@udf(name='prophet_fit', input_types=[ArrayType(StringType())], return_type=ArrayType(StringType()), is_permanent=False, replace=True)\n",
    "def prophet_fit(ds_y: list) -> list:\n",
    "    \n",
    "    #splitting column into dates and values\n",
    "    df = pd.DataFrame({'ds_y':ds_y}).ds_y.str.split('_',expand=True)\n",
    "    df.columns = ['ds', 'y']\n",
    "    df.ds = pd.to_datetime(df.ds)\n",
    "    df.y = df.y.astype(int)\n",
    "    \n",
    "    \n",
    "    # Enable daily sesanality since we are dealing with daily data\n",
    "    m = Prophet(daily_seasonality=True)\n",
    "    # Prophet has a built-in feature to easily add US holidays, so we will add that as a regressor\n",
    "    m.add_country_holidays(country_name='US')\n",
    "    \n",
    "    # Now we fit the model\n",
    "    m.fit(df)\n",
    "    \n",
    "    # This next step created a future facing data frame for 90 occurances (periods) at the daily level (freq)\n",
    "    future = m.make_future_dataframe(periods=90, freq='d')\n",
    "    \n",
    "    forecast = m.predict(future)\n",
    "    \n",
    "    \n",
    "    return forecast.ds.astype(str)+\"_\"+forecast.yhat.astype(str)+\"_\"+forecast.yhat_lower.astype(str)+\"_\"+forecast.yhat_upper.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3694563b-91af-40ae-bc55-c2c503404271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import array_agg\n",
    "\n",
    "#Aggregate training data by store-item\n",
    "train_rotated = train.groupBy('STORE_ITEM').agg([array_agg(col('DATE_SALES')).alias(\"ALL_DATE_SALES\")])\n",
    "\n",
    "#This requires more compute power to perform the fit, lets scale up processing power\n",
    "session.sql(\"alter warehouse data_science set warehouse_size=medium\").collect()\n",
    "\n",
    "#Run forecast\n",
    "forecasts = train_rotated.withColumn('FORECAST', prophet_fit('ALL_DATE_SALES')).select('STORE_ITEM','FORECAST').cache_result()\n",
    "#Takes 3min 40s on an XS\n",
    "#Takes 1min 14s on a M\n",
    "\n",
    "#Don't need the extra compute power anymore lets scale back down\n",
    "session.sql(\"alter warehouse data_science set warehouse_size=xsmall\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766b27c-1971-4cb2-96ec-db679aaf18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import split\n",
    "from snowflake.snowpark.functions import abs\n",
    "#Separate forecast values out\n",
    "flat_forecasts = forecasts.join_table_function(\"flatten\", col(\"FORECAST\")).select('STORE_ITEM','VALUE').withColumn('FORECASTS', split(col(\"VALUE\"), lit(\"_\")))\n",
    "\n",
    "final_forecasts = flat_forecasts.select('STORE_ITEM', flat_forecasts['FORECASTS'][0], flat_forecasts['FORECASTS'][1], flat_forecasts['FORECASTS'][2], flat_forecasts['FORECASTS'][3])\\\n",
    "                                .toDF([\"STORE_ITEM\",\"DATE\",\"YHAT\",\"YHAT_LOWER\",\"YHAT_UPPER\"])\\\n",
    "                                .withColumn(\"DATE\", col('DATE').cast(\"date\"))\\\n",
    "                                .withColumn(\"YHAT\", col('YHAT').cast(\"float\"))\\\n",
    "                                .withColumn(\"YHAT_LOWER\", col('YHAT_LOWER').cast(\"float\"))\\\n",
    "                                .withColumn(\"YHAT_UPPER\", col('YHAT_UPPER').cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371483a4-cfbb-4e58-9c50-fe23b60d7e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join back the actual sales values\n",
    "final_forecasts = final_forecasts.join(sales_sub, using_columns=['STORE_ITEM', 'DATE'], join_type='left')\n",
    "\n",
    "#Calculate forecast errors\n",
    "final_forecasts = final_forecasts.withColumn(\"ERROR\", col('SALES')-col('YHAT'))\\\n",
    "                                 .withColumn(\"ABS_ERROR\", abs(col('ERROR')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ae1c3-40ac-4527-92c7-79cd61f1868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a column that marks whether the row would be in train or test\n",
    "final_forecasts = final_forecasts.withColumn('EVAL_SET', when(col('DATE') <= \"2017-10-02\", \"TRAIN\").otherwise(\"TEST\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f69838-e30f-4abb-a499-d4b3f3e01551",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_forecasts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8418f9e0-ff0c-47ad-a354-91caaaafd592",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_forecasts.write.mode(\"overwrite\").save_as_table(\"OUTPUT_FORECASTS\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83376187",
   "metadata": {},
   "source": [
    "## Let's take a look at forecast accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70b37490",
   "metadata": {},
   "source": [
    "Most business define forecast accuracy as a weighted mean average percent error (MAPE).\n",
    "* Forecast Accuracy = 1 - ( ABS(Predicted - Actuals) / Actuals )\n",
    "\n",
    "We will use this approach to evaluate our forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fcd21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the forecast sales data from Snowflake \n",
    "results = session.table(\"OUTPUT_FORECASTS\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad466a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.withColumn('STORE', split(col(\"STORE_ITEM\"), lit(\"_\"))[0].cast(\"string\"))\\\n",
    "                 .withColumn('ITEM', split(col(\"STORE_ITEM\"), lit(\"_\"))[1].cast(\"string\"))\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eba7c2-3ae3-4046-830c-c123716fdc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results at an overall level (actuals and predicted)\n",
    "results_agg = results.select('DATE','SALES','YHAT').groupBy('DATE').agg([sum('YHAT').alias('YHAT'), sum('SALES').alias('SALES')]).sort(col('DATE').asc())\n",
    "results_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841450c4-58ac-4b1b-bcc4-9d397a62e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(14,8))\n",
    "ax.plot(results_agg.to_pandas()['DATE'], results_agg.to_pandas()['SALES'])\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Sales\")\n",
    "ax.plot(results_agg.to_pandas()['DATE'], results_agg.to_pandas()['YHAT'])\n",
    "plt.legend([\"Actual\", \"Predicted\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6089a43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = results.filter(col('EVAL_SET')=='TEST')\n",
    "eval_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9107c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_agg = eval_df.select('DATE','SALES','YHAT').groupBy('DATE').agg([sum('YHAT').alias('YHAT'), sum('SALES').alias('SALES')]).sort(col('DATE').asc())\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(14,8))\n",
    "ax.plot(results_agg.to_pandas()['DATE'], results_agg.to_pandas()['SALES'])\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Sales\")\n",
    "ax.plot(results_agg.to_pandas()['DATE'], results_agg.to_pandas()['YHAT'])\n",
    "plt.legend([\"Actual\", \"Predicted\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3777ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = results.filter(col('EVAL_SET')=='TRAIN')\n",
    "print('The forecast accuracy for the training set is: '+str(1-(train.agg(sum('ABS_ERROR')).collect()[0][0]/train.agg(sum('SALES')).collect()[0][0])))\n",
    "\n",
    "test = results.filter(col('EVAL_SET')=='TEST')\n",
    "print('The forecast accuracy for the testing set is: '+str(1-(test.agg(sum('ABS_ERROR')).collect()[0][0]/test.agg(sum('SALES')).collect()[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f720d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_item_accuracy = eval_df.select('STORE', 'ITEM', 'SALES', 'YHAT', 'ABS_ERROR')\\\n",
    "                             .groupBy(['STORE', 'ITEM']).agg([sum('ABS_ERROR').alias('ABS_ERROR'), sum('SALES').alias('SALES')])\\\n",
    "                             .withColumn('OVERALL_ACCURACY', lit(1)-(col('ABS_ERROR') / col('SALES')) )\n",
    "store_item_accuracy.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d3fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 store-item combinations by forecast accuracy\n",
    "store_item_accuracy.sort(col('OVERALL_ACCURACY').desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last 10 store-item combinations by forecast accuracy\n",
    "store_item_accuracy.sort(col('OVERALL_ACCURACY').asc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc1b624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close session results in Snowflake cleannig up all the temp session tables\n",
    "session.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9ec5781",
   "metadata": {},
   "source": [
    "Go into Snowflake query-history and see what's being done behind the scenes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysnowpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
